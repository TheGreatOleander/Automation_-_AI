<!DOCTYPE html>
<html lang="en">
<head>
<meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<title>Local Model Deployment with Ollama — AI Automation Manual v3.0</title>
<link rel="preconnect" href="https://fonts.googleapis.com">
<link href="https://fonts.googleapis.com/css2?family=IBM+Plex+Mono:ital@0;1&family=Oswald:wght@400;500;600&family=Lora:ital,wght@0,400;0,500;1,400&display=swap" rel="stylesheet">
<style>

  :root {
    --bg:       #0a0d12;
    --bg2:      #0f1318;
    --grid:     rgba(255,160,50,0.03);
    --orange:   #ff9a3c;
    --orange-dim:#7a4a1a;
    --gold:     #f0c040;
    --rust:     #c45a20;
    --text:     #d4c9bc;
    --text-dim: #5c5248;
    --border:   rgba(255,154,60,0.15);
    --code-bg:  #080b0f;
  }
  *, *::before, *::after { box-sizing: border-box; margin: 0; padding: 0; }
  html { scroll-behavior: smooth; }
  body {
    background: var(--bg);
    color: var(--text);
    font-family: 'Lora', serif;
    font-weight: 400;
    line-height: 1.8;
    min-height: 100vh;
  }
  body::before {
    content: '';
    position: fixed; inset: 0;
    background-image:
      linear-gradient(var(--grid) 1px, transparent 1px),
      linear-gradient(90deg, var(--grid) 1px, transparent 1px);
    background-size: 48px 48px;
    pointer-events: none; z-index: 0;
  }
  body::after {
    content: '';
    position: fixed; top: -300px; left: 50%;
    transform: translateX(-50%);
    width: 1000px; height: 600px;
    background: radial-gradient(ellipse, rgba(255,154,60,0.05) 0%, transparent 70%);
    pointer-events: none; z-index: 0;
  }

  /* ── TOP NAV ── */
  .topnav {
    position: sticky; top: 0; z-index: 100;
    background: rgba(10,13,18,0.94);
    backdrop-filter: blur(14px);
    border-bottom: 1px solid var(--border);
    padding: 12px 40px;
    display: flex; justify-content: space-between; align-items: center; gap: 16px;
  }
  .topnav-home {
    font-family: 'IBM Plex Mono', monospace;
    font-size: 11px; color: var(--orange);
    text-decoration: none; letter-spacing: 2px;
  }
  .topnav-home:hover { color: #fff; }
  .topnav-title {
    font-family: 'Oswald', sans-serif;
    font-size: 12px; font-weight: 400;
    color: var(--text-dim); text-transform: uppercase; letter-spacing: 2px;
    flex: 1; text-align: center;
  }
  .topnav-part {
    font-family: 'IBM Plex Mono', monospace;
    font-size: 11px; color: var(--orange-dim);
  }

  /* ── HERO ── */
  .hero {
    position: relative; z-index: 10;
    max-width: 900px; margin: 0 auto;
    padding: 72px 40px 52px;
    border-bottom: 1px solid var(--border);
  }
  .part-badge {
    font-family: 'IBM Plex Mono', monospace;
    font-size: 11px; color: var(--orange);
    letter-spacing: 4px; margin-bottom: 20px;
  }
  .part-badge::before { content: '// '; color: var(--orange-dim); }
  h1 {
    font-family: 'Oswald', sans-serif;
    font-weight: 600;
    font-size: clamp(28px, 5vw, 58px);
    line-height: 1.0; letter-spacing: -0.5px;
    text-transform: uppercase; color: #ffffff; margin-bottom: 16px;
  }
  .hero-sub {
    font-family: 'Lora', serif;
    font-style: italic; font-size: 15px;
    color: var(--text-dim); max-width: 580px;
  }

  /* ── ARTICLE ── */
  article {
    position: relative; z-index: 10;
    max-width: 900px; margin: 0 auto;
    padding: 60px 40px 96px;
  }
  h2 {
    font-family: 'Oswald', sans-serif;
    font-size: 20px; font-weight: 500;
    color: var(--orange); text-transform: uppercase; letter-spacing: 1px;
    margin: 56px 0 18px;
    padding-bottom: 10px;
    border-bottom: 1px solid var(--border);
  }
  h2:first-child { margin-top: 0; }
  h3 {
    font-family: 'Oswald', sans-serif;
    font-size: 15px; font-weight: 400;
    color: #e8ddd0; text-transform: uppercase; letter-spacing: 1.5px;
    margin: 36px 0 12px;
  }
  p { margin-bottom: 20px; font-size: 15.5px; }
  ul, ol { margin: 0 0 20px 0; padding-left: 0; list-style: none; }
  li {
    font-size: 15px; padding: 6px 0 6px 28px;
    position: relative;
    border-bottom: 1px solid rgba(255,154,60,0.04);
  }
  ul li::before {
    content: '▸'; position: absolute; left: 0;
    color: var(--orange-dim); font-size: 11px; top: 9px;
  }
  ol { counter-reset: item; }
  ol li { counter-increment: item; }
  ol li::before {
    content: counter(item) '.'; position: absolute; left: 0;
    color: var(--orange-dim);
    font-family: 'IBM Plex Mono', monospace; font-size: 11px; top: 9px;
  }
  pre {
    background: var(--code-bg);
    border: 1px solid rgba(255,154,60,0.1);
    border-left: 3px solid var(--rust);
    padding: 20px 24px; margin: 28px 0; overflow-x: auto;
    font-family: 'IBM Plex Mono', monospace;
    font-size: 12.5px; line-height: 1.75; color: #b8a898;
  }
  code {
    font-family: 'IBM Plex Mono', monospace;
    font-size: 0.88em; color: var(--orange); background: rgba(255,154,60,0.07);
    padding: 1px 5px; border-radius: 2px;
  }
  pre code { color: inherit; font-size: inherit; background: none; padding: 0; }
  .callout {
    margin: 28px 0; padding: 18px 22px;
    border-left: 3px solid var(--orange-dim);
    background: rgba(255,154,60,0.04);
    font-size: 14px; font-style: italic; color: var(--text-dim);
  }
  .callout.warn {
    border-left-color: var(--gold);
    background: rgba(240,192,64,0.04); color: #9a8040;
  }
  .callout.stop {
    border-left-color: #c84040;
    background: rgba(200,64,64,0.04); color: #904040;
  }
  .callout strong { color: var(--text); font-style: normal; }

  /* ── PAGE NAV ── */
  .page-nav {
    position: relative; z-index: 10;
    max-width: 900px; margin: 0 auto;
    padding: 32px 40px 72px;
    display: flex; justify-content: space-between; gap: 16px;
    border-top: 1px solid var(--border);
  }
  .nav-btn {
    font-family: 'IBM Plex Mono', monospace; font-size: 12px;
    color: var(--orange-dim); text-decoration: none;
    padding: 10px 18px; border: 1px solid var(--border); transition: all 0.2s;
  }
  .nav-btn:hover { color: var(--orange); border-color: var(--orange); background: rgba(255,154,60,0.05); }

  @media (max-width: 640px) {
    .topnav, .hero, article, .page-nav { padding-left: 20px; padding-right: 20px; }
    .topnav-title { display: none; }
  }

</style>
</head>
<body>

<nav class="topnav">
  <a class="topnav-home" href="../index.html">← INDEX</a>
  <div class="topnav-title">AI Automation Manual v3.0</div>
  <div class="topnav-part">PART XIX</div>
</nav>

<div class="hero">
  <div class="part-badge">PART XIX</div>
  <h1>Local Model Deployment with Ollama</h1>
</div>

<article>
<h2>19.1  When Local Models Are the Right Choice</h2>
<p>The case for local models is not primarily about cost — for most workflows, API pricing is cheaper than the infrastructure to run local models. The case is about data sovereignty, latency, and availability.</p>
<p>Deploy local models when: the data you are processing cannot leave your network (healthcare, legal, financial PII); you need sub-100ms inference latency and cannot tolerate API round trips; you are in an environment with unreliable internet connectivity; or you are processing high volumes of short, simple tasks where a small fast model outperforms a large slow one.</p>
<h2>19.2  Ollama: Local Model Management</h2>
<p>Ollama is the standard tool for running open-weight models locally. It handles model download, quantization selection, CUDA/Metal acceleration, and exposes an OpenAI-compatible API. A single command gets you a running model:</p>
<pre><code># Install Ollama (macOS/Linux)
curl -fsSL https://ollama.com/install.sh | sh</code></pre>
<pre><code># Pull and run a model
ollama pull llama3.2:3b           # Fast, 2GB RAM
ollama pull qwen2.5-coder:7b      # Code tasks, 4.5GB RAM
ollama pull gemma3:27b            # Higher quality, 16GB RAM</code></pre>
<pre><code># The Ollama API is OpenAI-compatible
curl http://localhost:11434/v1/chat/completions \
  -H &#x27;Content-Type: application/json&#x27; \
  -d &#x27;{&quot;model&quot;: &quot;llama3.2:3b&quot;, &quot;messages&quot;: [{&quot;role&quot;: &quot;user&quot;, &quot;content&quot;: &quot;Hello&quot;}]}&#x27;</code></pre>
<h2>19.3  Quantization and Model Selection</h2>
<p>Open-weight models are distributed in multiple quantization levels. Higher quantization (Q8, F16) preserves more of the model&#x27;s original quality but requires more memory and runs slower. Lower quantization (Q4, Q2) uses less memory and runs faster, with some quality loss.</p>
<p>The practical guide: for most automation tasks (classification, extraction, summarization), Q4_K_M hits the right balance of quality and speed. For code generation, use Q5_K_M or higher — code tasks are more sensitive to quantization artifacts. For creative or reasoning tasks, use Q8_0 if your hardware supports it.</p>
<p>Model families to know in 2026: Llama 3.x (Meta, strong general purpose), Qwen 2.5 (Alibaba, strong code and multilingual), Gemma 3 (Google, strong reasoning at smaller sizes), Mistral Nemo (Mistral, efficient and fast), Phi-4 (Microsoft, strong at small sizes for specific tasks).</p>
<h2>19.4  The Local-First Fallback Chain</h2>
<p>The most resilient architecture is a fallback chain: try the cloud model first, fall back to local on failure or high latency. This pattern gives you cloud quality under normal conditions and local reliability during outages.</p>
<pre><code>import httpx
from typing import Optional</code></pre>
<pre><code>async def call_with_fallback(prompt: str, prefer_local=False) -&gt; str:
    models = [
        (&#x27;local&#x27;, &#x27;http://localhost:11434/v1&#x27;, &#x27;llama3.2:3b&#x27;),
        (&#x27;cloud&#x27;, &#x27;https://api.anthropic.com/v1&#x27;, &#x27;claude-haiku-4-5&#x27;),
    ]
    if not prefer_local: models = list(reversed(models))</code></pre>
<pre><code>    for name, base_url, model in models:
        try:
            async with httpx.AsyncClient(timeout=10.0) as client:
                r = await client.post(f&#x27;{base_url}/chat/completions&#x27;,
                    json={&#x27;model&#x27;: model, &#x27;messages&#x27;: [{&#x27;role&#x27;:&#x27;user&#x27;,&#x27;content&#x27;:prompt}]})
                return r.json()[&#x27;choices&#x27;][0][&#x27;message&#x27;][&#x27;content&#x27;]
        except Exception as e:
            log.warning(f&#x27;{name} failed: {e}, trying next&#x27;)
    raise RuntimeError(&#x27;All models failed&#x27;)</code></pre>
<h2>19.5  Ollama in Docker Compose</h2>
<p>For a production setup, run Ollama as a Docker service alongside your other automation components. This ensures model availability is managed with the same tooling as the rest of your stack.</p>
<pre><code>services:
  ollama:
    image: ollama/ollama:latest
    volumes:
      - ollama_data:/root/.ollama
    ports: [&#x27;11434:11434&#x27;]
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: 1
              capabilities: [gpu]</code></pre>
<pre><code>  ollama_init:
    image: ollama/ollama:latest
    depends_on: [ollama]
    command: &gt;
      sh -c &#x27;sleep 5 &amp;&amp; ollama pull llama3.2:3b &amp;&amp; ollama pull qwen2.5-coder:7b&#x27;
    environment:
      - OLLAMA_HOST=http://ollama:11434</code></pre>
</article>

<div class="page-nav">
  <a href="part-18.html" class="nav-btn">← PART XVIII</a>
  <a href="part-20.html" class="nav-btn">PART XX →</a>
</div>

</body>
</html>